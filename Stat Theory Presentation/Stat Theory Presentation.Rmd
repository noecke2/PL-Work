---
title: "A Bayesian Look at Nonidentifiability: A Simple Example"
subtitle: "By Sergio Wechsler, Rafael Izbicki and Lu√≠s Gustavo Esteves"  
author: 
  - "Andrew Noecker"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#dc4c4c",
  secondary_color = "#157394",
  inverse_header_color = "#FFFFFF"
)
```

```{r libraries & data, include = FALSE, warning = FALSE}
library(tidyverse)
library(ggthemes)
library(protoclust)
library(ggdendro)
library(gt)
```



## The Basic Idea

- A model is identifiable IF:


$P_{\theta_1}(X = x) = P_{\theta_2}(X = x) \text{ implies that } \theta_1 = \theta_2$

--


- In other words, there can't be two different values of $\theta$ that generate identical likelihood functions.



- When we have a nonidentifiable model, we can take a Bayesian approach to develop a solution



<!-- Text can be **bold**, _italic_, ~~strikethrough~~, or `inline code`. -->

---

## How it relates to information

- Suppose parts in a factory defect for one of two reasons, which occur with probabilities $p_1 \text{ and } p_2$, respectively. 


- Then the probability of a part being adequate is $1 - p_1 - p_2$ and the probability of a part being defective is $p_1 + p_2$


- Now suppose we sample n items $\rightarrow X \sim Binom(n, p_1 + p_2)$


- No matter what data we observe, we will not be fully informed on $p_1$ and $p_2$ individually. 


---

## A Simple Example (Connecting to Book Club!)

- Suppose the following situation is presented:

  1. A missing plane has crashed in one of *n* possible regions

--

  2. A search is performed in region *i*, with probability $p_i$ the plane has crashed there and probability $\alpha_i$ that you recover the plane given the plane is in region *i*

--

  3. Our parameter of interest is $\theta$ - the region where the plane went down

--

  4. We observe our random variable $X$, where $X = \{0, 1\}$ - the plane is found in the region or it is not found. So, $P(X = 1 | \theta = i) = \alpha_i$

--

### Why this is nonidentifiable

If $\theta_1, \theta_2 \neq i$ (i.e the plane is not found in these regions), then $P(X = x | \theta_1) = P(X = x | \theta_2) = 1_{\{0\}}(x)$


---


## How Bayes can Help Address Nonidentiability

- We can use Bayes to derive a posterior distribution of X even in the absence of identifiability


$$ 
P(\theta = j | X = x) = \frac{P(\theta = j)P(X =x | \theta = j)}{\sum_{k=1}^n P(\theta = k)P(X = x | \theta = k)}
$$
- If $x = 1$, then we've found the plane and this is equal to 1. But if $x = 0$, then:




<center> 
$P(\theta = j | X = 0) = \frac{p_j}{1 - \alpha_ip_i}(1 - 1_{\{i\}}(j)) + \frac{(1 - \alpha_i)p_i}{1 - \alpha_ip_i} \cdot1_{\{i\}}(j)$
</center>



---

## Continuing with the Plane Example: The Importance of Priors

- Suppose there are 3 regions:

  - $p_1 = 70/100$, $p_2 = 25/100$, $p_3 = 5/100$
  
  - $\alpha_1 = 9/10$, $\alpha_2 = 5/10$, and $\alpha_3 = 9/10$

- If we search region 1 and do not find the plane (i.e $X = 0$), then our updated distribution is:

  - $P(\theta = 1 | X = 0) = 7/37$, 
  
  - $P(\theta = 2 | X = 0) = 25/37$
  
  - $P(\theta = 3 | X = 0) = 5 / 37$


---

## Some Comments on the Likelihood and Estimators

- The authors suggest the *mode* of the posterior as a simple estimator for $\theta$

- They also derive the MLE for $\theta$ 

  - If $X = 1$, then the MLE is simply $i$
  
  - If $X = 0$, then there are $n-1$ MLEs:
  
    - For $j \neq i$, then $\delta_j(X) = i1_1(X) + j1_0(X)$ are all MLE options for $\theta$
    
- Because we do not have MLEs that distinguish between parameter values, we confirm that we do not have unique likelihood functions and are thus dealing with nonidentifiability

- This shows how using Bayes has increased our knowledge of the situation (despite nonidentifiability): 

  - We can create a posterior distribution that tells us new information about $\theta$, which we wouldn't have been able to do using likelihoods. 


---

## Conclusions / Summary

- Even if our model is nonidentifiable, there may still be useful information about our parameters of interest, which we can extract using Bayes

- Because nonidentifiability leads to undesirable and nonunique MLEs, an expert's opinion is needed and preferred to uniform / objective priors. 


![JOY](joy.jpeg)
